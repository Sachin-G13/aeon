{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Demo for Time Series with aeon\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Deep learning is proven to be very effective for Time Series Classification (TSC) tasks after the extensive experiments done in [1], especially Convolution based architectures i.e. FCN and ResNet [2]. A while later, InceptionTime (Convolution based) was proposed to become the new state-of-the-art deep learning model for TSC [3]. In [4], new hand-crafted convolution filters were proposed to boost InceptionTime. The model proposed in [4], Hybrid InceptionTime (H-InceptionTime) is currently, at the time of writing, the state-of-the-art deep learning model for TSC.\n",
    "\n",
    "More recently, in the latest Time Series Regression (TSER) review [5], the deep learning model InceptionTime is seen to be the state-of-the-art deep learning model.\n",
    "\n",
    "Moreover, deep learning models seem to work quite well to solve the Time Series Clustering task (TSCL).\n",
    "The models are from the deep learning clustering bake off [7] (more models from that bake off will soon be in aeon).\n",
    "\n",
    "In this demo, we cover the usage of the deep learning models for both TSC, TSCL and TSER.\n",
    "\n",
    "Soon we plan to include more tasks into the deep learning domain suchas Time Series Forecasting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. Imports\n",
    "2. Classification\n",
    "    - InceptionTime\n",
    "    - H-InceptionTime\n",
    "3. Regression\n",
    "    - InceptionTime\n",
    "4. Clustering\n",
    "    - Auto-Encoder Fully Convolutional Network\n",
    "5. Saving and Loading model\n",
    "6. Deep Learning for any time series task\n",
    "7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports\n",
    "\n",
    "Import the deep learning models for classification and regression, the data loader and InceptionNetwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# avoid imports warning for tensorflow\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "# avoid packages warnings such as sklearn\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# skip future tensorflow warning messages\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras import backend as k\n",
    "\n",
    "from aeon.classification.deep_learning import FCNClassifier, InceptionTimeClassifier\n",
    "from aeon.clustering.deep_learning import AEFCNClusterer\n",
    "from aeon.datasets import load_classification, load_regression\n",
    "from aeon.networks import InceptionNetwork\n",
    "from aeon.regression.deep_learning import InceptionTimeRegressor"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classification\n",
    "\n",
    "#### InceptionTime\n",
    "\n",
    "The InceptionTime model is an ensemble of multiple (by default five) Inception models. Each Inception model is a Convolutional Neural Network made of six Inception modules as seen in the Figure below. Each Inception module is composed of multiple (by default three) convolution layer in parallel and a max-pooling operation as well.\n",
    "![InceptionTime](./img/Inception.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "xtrain, ytrain = load_classification(name=\"ArrowHead\", split=\"train\")\n",
    "xtest, ytest = load_classification(name=\"ArrowHead\", split=\"test\")\n",
    "\n",
    "inc = InceptionTimeClassifier(n_classifiers=5, use_custom_filters=False, n_epochs=3)\n",
    "inc.fit(X=xtrain, y=ytrain)\n",
    "ypred = inc.predict(X=xtest)\n",
    "\n",
    "print(\"Predictions: \", ypred[0:5])\n",
    "print(\"Ground Truth: \", ytest[0:5])"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H-InceptionTime\n",
    "\n",
    "Just as InceptionTime, H-InceptionTime is an ensemble of multiple H-Inception models. The model can be seen in the figure below, where the additional custom filters are added in parallel to the first Inception module.\n",
    "![H-InceptionTime](./img/H-Inception.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of H-InceptionTime can be done by setting the ```use_custom_filters``` flag to True. This is the default setup, not setting this flag to True will results in using the H-InceptionTime by default and only by setting it to False that InceptionTime is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "xtrain, ytrain = load_classification(name=\"ArrowHead\", split=\"train\")\n",
    "xtest, ytest = load_classification(name=\"ArrowHead\", split=\"test\")\n",
    "\n",
    "inc = InceptionTimeClassifier(n_classifiers=5, use_custom_filters=True, n_epochs=3)\n",
    "inc.fit(X=xtrain, y=ytrain)\n",
    "ypred = inc.predict(X=xtest)\n",
    "\n",
    "print(\"Predictions: \", ypred[0:5])\n",
    "print(\"Ground Truth: \", ytest[0:5])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regression\n",
    "\n",
    "#### InceptionTime\n",
    "\n",
    "As for classification, the InceptionTime model for TSR tasks is an ensemble of five different Inception models. The difference here is just in the last layer where the number of output neurons is one and no activation is used. The loss function used in this case is the Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "xtrain, ytrain = load_regression(name=\"Covid3Month\", split=\"train\")\n",
    "xtest, ytest = load_regression(name=\"Covid3Month\", split=\"test\")\n",
    "\n",
    "inc = InceptionTimeRegressor(n_regressors=5, n_epochs=1, use_custom_filters=False)\n",
    "inc.fit(X=xtrain, y=ytrain)\n",
    "ypred = inc.predict(X=xtest)\n",
    "\n",
    "print(\"Predictions: \", ypred[0:5])\n",
    "print(\"Ground Truth: \", ytest[0:5])"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clustering\n",
    "\n",
    "#### Auto-Encoder Fully Convolutional Network\n",
    "\n",
    "For this model, we adapted the idea in [7] to construct a simple auto-encoder model with the Fully Convolutional Network (FCN) [2] as backbone for the encoder and decoder architectures. Simply the decoder is the symetrical version of the encoder (FCN) with the usage of Convolution1DTranspose instead of normal Convolution1D layers. The auto-encoder is trained to correcly reconstruct the input time series in order to learn a clustering task in the latent representations (between encoder and decoder).\n",
    "\n",
    "The AEFCNClusterer can be used in many ways, first of all, you can control whether your latent space is time series space or Euclidean space.\n",
    "This is done by setting the `temporal_latent_space` flag to `True` or `False`.\n",
    "\n",
    "Given that the AEFCNClusterer learns the clustering task in the latent space, then a clustering algorithm should be defined, this is done by passing it as string value to thr `clustering_algorithm` parameter. Modifying the clustering algorithm parameters is possible throught a dictionary parameter `clustering_params`.\n",
    "\n",
    "To train the model, simply call the `fit()` function which will train the auto-encoder followed by training the clustering algorithm in the latent space.\n",
    "\n",
    "In what follows we show two examples in using AEFCNClusterer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "xtrain, _ = load_classification(name=\"ArrowHead\", split=\"train\")\n",
    "xtest, ytest = load_classification(name=\"ArrowHead\", split=\"test\")\n",
    "\n",
    "aefcn = AEFCNClusterer(\n",
    "    n_clusters=2,\n",
    "    temporal_latent_space=False,\n",
    "    clustering_algorithm=\"kmeans\",\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "aefcn.fit(X=xtrain)\n",
    "ypred = aefcn.predict(X=xtest)\n",
    "print(\"Predictions: \", ypred[0:5])\n",
    "print(\"Ground Truth: \", ytest[0:5])\n",
    "print()\n",
    "print(\"Score : \", aefcn.score(X=xtest))"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Saving and Loading Model\n",
    "\n",
    "#### Saving model\n",
    "\n",
    "While training a deep learning model with `aeon`, a default callback is used to save the best model during training based on the training loss. This saved model is then loaded to use for evaluation and deleted once no longer used. This intermediate step is to avoid saving the model to file when the user is not in need of it later. This however can be changed by setting the `save_best_model` flag to True. The default name for the model is \"best_model\" with a `.keras` extension and will be saved in the default working directory. To change this, simply change the `file_path` and `best_file_name` parameters to your preference.\n",
    "\n",
    "The same settings can be used if the user needs to save the last model during training as well. This is done by simply setting the flag `save_last_model` to True. The name of this saved model can be set by changing the `last_file_name` parameter and the `file_path` as well.\n",
    "\n",
    "Here is an example on saving the best and last models of an FCN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "xtrain, ytrain = load_classification(name=\"ArrowHead\", split=\"train\")\n",
    "xtest, ytest = load_classification(name=\"ArrowHead\", split=\"test\")\n",
    "\n",
    "fcn = FCNClassifier(\n",
    "    save_best_model=True,\n",
    "    save_last_model=True,\n",
    "    file_path=\"./\",\n",
    "    best_file_name=\"best_fcn\",\n",
    "    last_file_name=\"last_fcn\",\n",
    "    n_epochs=2,\n",
    ")\n",
    "\n",
    "# The following is commented to avoid CI doing the saving for no reason\n",
    "\n",
    "# fcn.fit(X=xtrain, y=ytrain)\n",
    "# ypred = fcn.predict(X=xtest)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Deep Learning for any time series task\n",
    "\n",
    "Suppose you want to use a single Inception Network, for example to perform some self-supervised learning.  This is simple in aeon because of the structure on the network module. You can create any of model to perform any task using any of the Neural Networks in the module.\n",
    "\n",
    "In this Example we will be re-creating the TRILITE Self-Supervised Learning model from [6]. This model uses a deep learning encoder coupled with a triplet loss mechanism to learn a compact latent representation of the time series data. This model require a loss that is not pre-implented in deep learning frameworks (such as tensorflow, aeon's deep learning backend) and a triplets generation technique.\n",
    "The approach uses the loss and triplets to optimizer the parameters of a deep learning encoder so aeon can be used to create this encoder's architecture and the rest is from the user side as in the rest of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# define self-supervised space dimension\n",
    "\n",
    "n_dim = 16\n",
    "\n",
    "# load the data\n",
    "\n",
    "xtrain, ytrain = load_classification(name=\"ArrowHead\", split=\"train\")\n",
    "xtest, ytest = load_classification(name=\"ArrowHead\", split=\"test\")\n",
    "\n",
    "# Flip axis to be handled correctly in tensorflow\n",
    "\n",
    "xtrain = np.transpose(xtrain, axes=(0, 2, 1))\n",
    "xtest = np.transpose(xtest, axes=(0, 2, 1))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the triplet loss needed for the SSL model. Taken from [5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "def triplet_loss_function(alpha):\n",
    "    def temp(ytrue, ypred):\n",
    "        ref = ypred[:, :, 0]\n",
    "        pos = ypred[:, :, 1]\n",
    "        neg = ypred[:, :, 2]\n",
    "\n",
    "        ref = k.cast(ref, dtype=ref.dtype)\n",
    "        pos = k.cast(pos, dtype=ref.dtype)\n",
    "        neg = k.cast(neg, dtype=ref.dtype)\n",
    "\n",
    "        loss = k.maximum(\n",
    "            k.sum(\n",
    "                tf.math.subtract(\n",
    "                    tf.math.add(k.square(ref - pos), alpha), k.square(ref - neg)\n",
    "                ),\n",
    "                axis=1,\n",
    "            ),\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    return temp"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the network with the output self-supervised projection layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# Define the triplets input layers\n",
    "\n",
    "\n",
    "input_ref_layer = tf.keras.layers.Input(xtrain.shape[1:])\n",
    "input_pos_layer = tf.keras.layers.Input(xtrain.shape[1:])\n",
    "input_neg_layer = tf.keras.layers.Input(xtrain.shape[1:])\n",
    "\n",
    "inc_network = InceptionNetwork()\n",
    "input_layer, gap_layer = inc_network.build_network(input_shape=xtrain.shape[1:])\n",
    "\n",
    "encoder = tf.keras.models.Model(inputs=input_layer, outputs=gap_layer)\n",
    "\n",
    "output_layer_ref = tf.keras.layers.Reshape(target_shape=(-1, 1))(\n",
    "    tf.keras.layers.Dense(units=n_dim)(encoder(input_ref_layer))\n",
    ")\n",
    "output_layer_pos = tf.keras.layers.Reshape(target_shape=(-1, 1))(\n",
    "    tf.keras.layers.Dense(units=n_dim)(encoder(input_pos_layer))\n",
    ")\n",
    "output_layer_neg = tf.keras.layers.Reshape(target_shape=(-1, 1))(\n",
    "    tf.keras.layers.Dense(units=n_dim)(encoder(input_neg_layer))\n",
    ")\n",
    "\n",
    "output_layer = tf.keras.layers.Concatenate(axis=-1)(\n",
    "    [output_layer_ref, output_layer_pos, output_layer_neg]\n",
    ")\n",
    "\n",
    "SSL_model = tf.keras.models.Model(\n",
    "    inputs=[input_ref_layer, input_pos_layer, input_neg_layer], outputs=output_layer\n",
    ")\n",
    "\n",
    "SSL_model.compile(loss=triplet_loss_function(alpha=1e-5))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your triplets beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "def triplet_generation(x):\n",
    "    w1 = np.random.choice(np.linspace(start=0.6, stop=1, num=1000), size=1)\n",
    "    w2 = (1 - w1) / 2\n",
    "\n",
    "    ref = np.random.permutation(x[:])\n",
    "\n",
    "    n = int(ref.shape[0])\n",
    "\n",
    "    pos = np.zeros(shape=ref.shape)\n",
    "    neg = np.zeros(shape=ref.shape)\n",
    "\n",
    "    all_indices = np.arange(start=0, stop=n)\n",
    "\n",
    "    for i in range(n):\n",
    "        temp_indices = np.delete(arr=all_indices, obj=i)\n",
    "        indice_neg = int(np.random.choice(temp_indices, size=1))\n",
    "\n",
    "        temp_indices = np.delete(arr=all_indices, obj=[i, indice_neg])\n",
    "        indice_b = int(np.random.choice(temp_indices, size=1))\n",
    "\n",
    "        indice_c = int(np.random.choice(temp_indices, size=1))\n",
    "\n",
    "        indice_b2 = int(np.random.choice(temp_indices, size=1))\n",
    "\n",
    "        indice_c2 = int(np.random.choice(temp_indices, size=1))\n",
    "\n",
    "        a = ref[i].copy()\n",
    "\n",
    "        nota = ref[indice_neg].copy()\n",
    "\n",
    "        b = ref[indice_b].copy()\n",
    "        c = ref[indice_c].copy()\n",
    "\n",
    "        b2 = ref[indice_b2].copy()\n",
    "        c2 = ref[indice_c2].copy()\n",
    "\n",
    "        # MixingUp\n",
    "\n",
    "        pos[i] = w1 * a + w2 * b + w2 * c\n",
    "        neg[i] = w1 * nota + w2 * b2 + w2 * c2\n",
    "\n",
    "    return ref, pos, neg"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "xtrain_ref, xtrain_pos, xtrain_neg = triplet_generation(x=xtrain)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the model's callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"loss\", factor=0.5, patience=50, min_lr=0.0001\n",
    ")\n",
    "\n",
    "file_path = \"./best_ssl_model.keras\"\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=file_path, monitor=\"loss\", save_best_only=True\n",
    ")\n",
    "\n",
    "callbacks = [reduce_lr, model_checkpoint]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "history = SSL_model.fit(\n",
    "    [xtrain_ref, xtrain_pos, xtrain_neg],\n",
    "    np.zeros(shape=len(xtrain)),\n",
    "    epochs=20,\n",
    "    callbacks=callbacks,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], lw=3, color=\"blue\", label=\"training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the 2D representation of the train data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "plt.figure()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# load best model\n",
    "model = tf.keras.models.load_model(\"./best_ssl_model.keras\", compile=False)\n",
    "latent_space = model.predict([xtrain, xtrain, xtrain])\n",
    "latent_space = latent_space[:, :, 0]\n",
    "\n",
    "pca.fit(latent_space)\n",
    "latent_space_2d = pca.transform(latent_space)\n",
    "\n",
    "colors = [\"blue\", \"red\", \"green\"]\n",
    "\n",
    "for c in range(len(np.unique(ytrain))):\n",
    "    plt.scatter(\n",
    "        latent_space_2d[ytrain.astype(np.int64) == c][:, 0],\n",
    "        latent_space_2d[ytrain.astype(np.int64) == c][:, 1],\n",
    "        color=colors[c],\n",
    "        label=\"class-\" + str(c),\n",
    "    )\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "os.remove(\"./best_ssl_model.keras\")"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. References\n",
    "\n",
    "- [1] Ismail Fawaz, Hassan, et al. \"Deep learning for time series classification: a\n",
    "    review.\" Data mining and knowledge discovery 33.4 (2019): 917-963.\n",
    "- [2] Wang, Zhiguang et al. \"Time series classification from scratch with deep neural\n",
    "    networks: A strong baseline.\" 2017 International joint conference on neural networks (IJCNN). IEEE, 2017.\n",
    "- [3] Ismail Fawaz, Hassan, et al. \"Inceptiontime: Finding alexnet for time series\n",
    "    classification.\" Data Mining and Knowledge Discovery 34.6 (2020): 1936-1962.\n",
    "- [4] Ismail-Fawaz, Ali, et al. \"Deep Learning For Time Series Classification Using New\n",
    "    Hand-Crafted Convolution Filters.\" International Conference on Big Data. IEEE, (2022).\n",
    "- [5] Guijo-Rubio, David, et al. \"Unsupervised Feature Based Algorithms for Time Series\n",
    "    Extrinsic Regression.\" arXiv preprint arXiv:2305.01429 (2023).\n",
    "- [6] Ismail-Fawaz, Ali, et al. \"Enhancing Time Series Classification with Self-Supervised Learning.\"\n",
    "    International Conference on Agents and Artificial Intelligence ICAART (2023).\n",
    "- [7] Lafabregue, Baptiste, et al. \"End-to-end deep representation learning for time series clustering:\n",
    "    a comparative study.\" Data Mining and Knowledge Discovery 36.1 (2022): 29-81."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('aeon-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cab4a84a7e2d58e8dd6743083175e639fa658fd5115758e2b424950b9bcc46f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
