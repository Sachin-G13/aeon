{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Benchmarking time series regression models\n",
    "\n",
    "Time series extrinsic regression, first properly defined in [1] then recently\n",
    "extended in [2], involves predicting a continuous target variable based on a time\n",
    "series. It differs from time series forecasting regression in that the target is\n",
    "not formed from a sliding window, but is some external variable.\n",
    "\n",
    "This notebook shows you how to use aeon to get benchmarking datasets with aeon and how\n",
    " to compare results on these datasets with those published in [2]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading/Downloading data\n",
    "\n",
    "aeon comes with two regression problems in the datasets module. You can load these\n",
    "with single problem loaders or the more general load_regression function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from aeon.datasets import load_cardano_sentiment, load_covid_3month, load_regression\n",
    "\n",
    "trainX, trainy = load_covid_3month(split=\"train\")\n",
    "testX, testy = load_regression(split=\"test\", name=\"Covid3Month\")\n",
    "X, y = load_cardano_sentiment()  # Combines train and test splits\n",
    "print(trainX.shape, testX.shape, X.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "there are currently 63 problems in the TSER archive hosted on\n",
    "timeseriesclassification.com. These are listed in the file datasets.tser_datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from aeon.datasets.tser_datasets import tser_soton\n",
    "\n",
    "print(sorted(list(tser_soton)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can download these datasets directly with aeon load_regression function. By\n",
    "default it will store the data in a directory called \"local_data\" in the datasets\n",
    "module. Set ``extract_path`` to specify a different location.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "small_problems = [\n",
    "    \"CardanoSentiment\",\n",
    "    \"Covid3Month\",\n",
    "]\n",
    "\n",
    "for problem in small_problems:\n",
    "    X, y = load_regression(name=problem)\n",
    "    print(problem, X.shape, y.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This stores the data in a format like this\n",
    "\n",
    "If you call the function again, it will load\n",
    "from disk rather than downloading\n",
    "again.\n",
    " You can specify train/test splits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "for problem in small_problems:\n",
    "    trainX, trainy = load_regression(name=problem, split=\"train\")\n",
    "    print(problem, X.shape, y.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating a regressor on benchmark data\n",
    "\n",
    "With the data, it is easy to assess an algorithm performance. We will use the\n",
    "DummyRegressor as a baseline, and the default scoring\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from aeon.regression import DummyRegressor\n",
    "\n",
    "dummy = DummyRegressor()\n",
    "performance = []\n",
    "for problem in small_problems:\n",
    "    trainX, trainy = load_regression(name=problem, split=\"train\")\n",
    "    dummy.fit(trainX, trainy)\n",
    "    testX, testy = load_regression(name=problem, split=\"test\")\n",
    "    predictions = dummy.predict(testX)\n",
    "    mse = mean_squared_error(testy, predictions)\n",
    "    performance.append(mse)\n",
    "    print(problem, \" Dummy score = \", mse)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparing to published results\n",
    "\n",
    "How does the dummy compare to the published results in [2]? We can use the method\n",
    "get_estimator_results to obtain published results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from aeon.benchmarking import get_available_estimators, get_estimator_results\n",
    "\n",
    "print(get_available_estimators(task=\"regression\"))\n",
    "results = get_estimator_results(\n",
    "    estimators=[\"DrCIF\", \"FreshPRINCE\"],\n",
    "    task=\"regression\",\n",
    "    datasets=small_problems,\n",
    "    measure=\"mse\",\n",
    ")\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "this is organised as a dictionary of dictionaries. because we cannot be sure all\n",
    "results are present for all datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "from aeon.benchmarking import get_estimator_results_as_array\n",
    "\n",
    "results, names = get_estimator_results_as_array(\n",
    "    estimators=[\"DrCIF\", \"FreshPRINCE\"],\n",
    "    task=\"regression\",\n",
    "    datasets=small_problems,\n",
    "    measure=\"mse\",\n",
    ")\n",
    "print(results)\n",
    "print(names)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "we just need to align our results from the website so they are aligned with the\n",
    "results from our dummy regressor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "import numpy as np\n",
    "\n",
    "paired_sorted = sorted(zip(names, results))\n",
    "names, _ = zip(*paired_sorted)\n",
    "sorted_rows = [row for _, row in paired_sorted]\n",
    "sorted_results = np.array(sorted_rows)\n",
    "print(names)\n",
    "print(sorted_results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do the same for our dummy regressor results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "paired = sorted(zip(small_problems, performance))\n",
    "small_problems, performance = zip(*paired)\n",
    "print(small_problems)\n",
    "print(performance)\n",
    "all_results = np.column_stack((sorted_results, performance))\n",
    "print(all_results)\n",
    "regressors = [\"DrCIF\", \"FreshPRINCE\", \"Dummy\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparing Regressors\n",
    "\n",
    "aeon provides visualisation tools to compare regressors.\n",
    "\n",
    "## Comparing two regressors\n",
    "\n",
    "We can plot the results against each other. This also presents the wins and losses\n",
    "and some summary statistics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from aeon.visualisation import plot_pairwise_scatter\n",
    "\n",
    "fig, ax = plot_pairwise_scatter(\n",
    "    all_results[:, 1],\n",
    "    all_results[:, 2],\n",
    "    \"FreshPRINCE\",\n",
    "    \"Dummy\",\n",
    "    metric=\"mse\",\n",
    "    lower_better=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Comparing multiple regressors\n",
    "\n",
    "We can plot the results of multiple regressors on a critical difference diagram,\n",
    "which shows the average rank and groups estimators by whether they are significantly\n",
    "different from each other."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from aeon.visualisation import plot_critical_difference\n",
    "\n",
    "res = plot_critical_difference(\n",
    "    all_results,\n",
    "    regressors,\n",
    "    lower_better=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "from aeon.visualisation import plot_boxplot_median\n",
    "\n",
    "res = plot_boxplot_median(\n",
    "    all_results,\n",
    "    regressors,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
