{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Partition based time series clustering in aeon\n",
    "\n",
    "Partition based clustering algorithms for time series are those where $k$\n",
    "clusters are created from $n$ time series. The aim is to cluster so that each time\n",
    "series in a cluster are homogenous (similar) to each other and heterogeneous\n",
    "(dissimilar) to those outside the cluster.\n",
    "\n",
    "Broadly speaking, clustering algorithms are either partitional or hierarchical. Partitional clustering algorithms assign (possibly probabilistic) cluster membership to each time series, usually through an iterative heuristic process of optimising some objective function that measures homogeneity.  Given a dataset of $n$ time series, $D$, the partitional time series clustering problem is to partition $D$ into $k$ clusters, $C = \\{C_1, C_2, ..., C_k\\}$ where $k$ is the number of clusters.\n",
    "\n",
    "To measure homogeneity and place a time series into a cluster, a metric or distance is\n",
    "used. For details of distances in aeon see the [distances examples](../distances/distances.ipynb). The distance is used to\n",
    "compare a time series and assign cluster membership.\n",
    "\n",
    "It is usually assumed that the number of clusters is set prior to the optimisation heuristic. If $k$ is not set, it is commonly found through some wrapper method. We assume $k$ is fixed in advance for all our experiments.\n",
    "\n",
    "Currently, in aeon we have $k$-means with alternative averaging methods and\n",
    "$k$-medoids with alternative medoids selection algorithms, and wrappers for tslearn\n",
    "$k$-shapes and kernel $k$-means.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Contents\n",
    "1. $k$-partition clustering algorithms\n",
    "1.1 Representative initialisation\n",
    "1.2 Assignment (distance measure)\n",
    "2. $k$-means clustering\n",
    "2.1 Updating centroids\n",
    "2.2 Example usage of $k$-means\n",
    "3. $k$-medoids clustering\n",
    "3.1. Partitioning Around the Medoids (PAM)\n",
    "3.2. Alternating $k$-medoids (Lloyds)\n",
    "3.3. Clustering LARge Applications (CLARA)\n",
    "3.4. CLARA based raNdomised Search (CLARANS)\n",
    "4. Performance comparison\n",
    "5. Recreating results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Imports and load data\n",
    "from aeon.clustering import TimeSeriesKMeans, TimeSeriesKMedoids\n",
    "from aeon.datasets import load_unit_test\n",
    "from aeon.visualisation import plot_cluster_algorithm\n",
    "\n",
    "X_train, y_train = load_unit_test(split=\"train\")\n",
    "(\n",
    "    X_test,\n",
    "    y_test,\n",
    ") = load_unit_test(split=\"test\")\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:14.140372938Z",
     "start_time": "2023-09-24T22:34:13.341739012Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. $k$-partition clustering algorithms\n",
    "\n",
    "$k$-partition clustering algorithms are those that seek to create $k$ clusters and\n",
    "assign each time series to a cluster using a distance measure. A representative\n",
    "series is created or selected to represent the centre of the cluster. In aeon the\n",
    "following partitional clustering algorithms are currently supported:\n",
    "\n",
    "- $k$-means - one of the most well known clustering algorithms. The goal of\n",
    "the $k$-means algorithm is to minimise a criterion known as the inertia or\n",
    "within-cluster sum-of-squares (uses the mean of the values in a cluster as\n",
    "the centre). Each cluster is represented by some form of average of the cluster\n",
    "members (the centroid).\n",
    "\n",
    "- $k$-medoids - A similar algorithm to $k$-means but instead of using the average of\n",
    "each cluster member to determine the centre, a representative time series is used\n",
    "(the medoid).\n",
    "\n",
    "Most partitional clustering algorithms follow an iterative greedy search involving\n",
    "the following steps:\n",
    "- Initialisation of cluster representatives\n",
    "- Assignment of series to clusters based on distance to representatives\n",
    "- Updating representatives to reflect new clusters\n",
    "- Repeat assigment and updating until convergence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Representative initialisation\n",
    "\n",
    "Representative initialisation is the first step and has been found to be critical in\n",
    "obtaining good results. There are three commonly used initialisation strategies\n",
    "available in aeon through the constructor argument `init_algorithm`:\n",
    "<ul>\n",
    "    <li> Representative initialisation supported for $k$-means and $k$-medoids:\n",
    "        <ul>\n",
    "            <li>Random - This is where each sample in the training dataset is\n",
    "            randomly assigned to a cluster and the update function is run based on\n",
    "            these random assignments i.e. for k-means the mean of these\n",
    "            randomly assigned clusters is used and for k-medoids the median of\n",
    "            these randomly assigned clusters is used as the initial representative.</li>\n",
    "            <li>Forgy - This simply selects $k$ random samples from the\n",
    "            training dataset as the representatives.</li>\n",
    "            <li>K-means++ - This is where the first representative is randomly chosen\n",
    "            from the training dataset. The distance between all series and the first\n",
    "           representative is found. The next representative is then chosen from the\n",
    "            with probability proportional to the distance to the first representative (i\n",
    "            .e. the greater the distance the more likely the time series will be\n",
    "            chosen). The remaining\n",
    "            centres are selected using the same method, using the nearest distance to\n",
    "             representatives already chosen to find the probability of selection.\n",
    "             This means that the selected representatives will proably be further\n",
    "             apart from each other than randomly selected representatives.\n",
    "            </li>\n",
    "         </ul>\n",
    "     </li>\n",
    " </ul>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "temp = TimeSeriesKMeans(\n",
    "    init_algorithm=\"kmeans++\",  # initialisation technique: random, or kmeans++\n",
    ")\n",
    "print(temp.init_algorithm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:15.717217498Z",
     "start_time": "2023-09-24T22:34:15.704620193Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Assignment using a distance measure\n",
    "How a time series is assigned to a cluster is based on its distance from it to the\n",
    "cluster representatives. This means for some $k$-partition approaches, different\n",
    "distance functions can be used to get different results. The following distance\n",
    "functions are supported, and the constructor parameter value are  as follows:\n",
    "<ul>\n",
    "    <li>K-means, K-medoids supported distances:\n",
    "        <ul>\n",
    "            <li>Euclidean - parameter string 'euclidean'</li>\n",
    "            <li>Manhattan - parameter string 'manhattan'</li>\n",
    "            <li>DTW - parameter string 'dtw'</li>\n",
    "            <li>DDTW - parameter string 'ddtw'</li>\n",
    "            <li>WDTW - parameter string 'wdtw'</li>\n",
    "            <li>WDDTW - parameter string 'wddtw'</li>\n",
    "            <li>LCSS - parameter string 'lcss'</li>\n",
    "            <li>ERP - parameter string 'erp'</li>\n",
    "            <li>EDR - parameter string 'edr'</li>\n",
    "            <li>MSM - parameter string 'msm'</li>\n",
    "            <li>TWE - parameter string 'twe'</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "The distance function can be set with the constructor argument `metric` for $k$-means\n",
    " and `distance` for $k$-medoids (we will unify this soon). The\n",
    "distance function parameters can be set by passing a dictionary to\n",
    "`distance_params`. Different distance functions may have different\n",
    "distance parameters. Please see the [distances notebook](../distances/distances.ipynb) for more info\n",
    " on these functions and the [distance based clustering bake off](https://arxiv.org/abs/2205.15181) for further background and examples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. K-means clustering\n",
    "\n",
    "One of the most popular TSCL approaches. The goal of the k-means algorithm is to\n",
    "minimise a criterion known as the inertia or within-cluster sum-of-squares (uses the\n",
    "mean of the values in a cluster as the centre). The best distance to use with k-means is MSM [1]_ and the best averaging technique to use is MBA (to do this see\n",
    "above it is specified how to use barycentre averaging with MSM for k-means) [2]_.\n",
    "Basic usage with DTW distance with a window parameter of 0.25 as follows:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "TimeSeriesKMeans(\n",
    "    distance=\"dtw\",  # DTW distance with 25% warping allowed\n",
    "    distance_params={\"window\": 0.25},\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:18.447582832Z",
     "start_time": "2023-09-24T22:34:18.419063428Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Updating centroids\n",
    "\n",
    "Updating centre representatives is key to how an algorithm improves the clustering.\n",
    "It is also the most important difference between $k$-means and $k$-medoids.\n",
    "$k$-means averages the current cluster members to update the centroid using one of\n",
    "two methods:\n",
    "<ul>\n",
    "    <li> Mean average - This is a standard mean average creating a new\n",
    "             series that is that average of all the series inside the cluster.\n",
    "             If the metric is Euclidean distance, this makes Ideal when using euclidean\n",
    "             distance. Can be specified to use\n",
    "            by passing 'means' as the parameter for `averaging_algorithm` to\n",
    "             k-means</li>\n",
    "    <li> Barycentre averaging (BA) - This is a specialised\n",
    "            averaging metric that is intended to be used with elastic\n",
    "            distance measures. BA involves forming an alignment path between all\n",
    "            cluster members and the current centroid. It then finds the values\n",
    "            warped to each point and averages them, thus forming a new centroid. In\n",
    "            principle, any elastic distance can be used with BA, although it is best\n",
    "            known for use with Dynamic Time\n",
    "            Warping (DTW) [6]. We also support its use with all distances by passing\n",
    "            'ba' as the parameter for `averaging_algorithm` .\n",
    "            </li>\n",
    "</ul>\n",
    "\n",
    "Note it is possible to mix and match distance functions and averaging methods,\n",
    "although not advisable. The averaging method can be set with the `averaging_method`\n",
    "for $k$-means. This can be a callable, or a string, that should be one of `mean`,\n",
    "or`ba`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "temp = TimeSeriesKMeans(distance=\"euclidean\", averaging_method=\"mean\")\n",
    "print(temp.get_params())\n",
    "TimeSeriesKMeans(\n",
    "    distance=\"msm\", averaging_method=\"ba\", average_params={\"max_iters\": 10}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:19.950404843Z",
     "start_time": "2023-09-24T22:34:19.943371384Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.2 Example usage of $k$-means\n",
    "\n",
    "We now put it all together to demonstrate how to use $k$-means. In addition\n",
    "to the above parameters, $k$-means also has `max_iter` and `random_state` parameters.\n",
    " `max_iter` specifies the maximum number of iterations to allow.  `random_state`\n",
    " seeds the clusterer to make it deterministic."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "k_means = TimeSeriesKMeans(\n",
    "    n_clusters=2,  # Number of desired centers\n",
    "    init_algorithm=\"random\",  # initialisation technique: random, first or kmeans++\n",
    "    max_iter=10,  # Maximum number of iterations for refinement on training set\n",
    "    distance=\"dtw\",  # Distance metric to use\n",
    "    averaging_method=\"mean\",  # Averaging technique to use\n",
    "    random_state=1,  # Makes deterministic\n",
    ")\n",
    "\n",
    "k_means.fit(X_train)\n",
    "plot_cluster_algorithm(k_means, X_test, k_means.n_clusters)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:22.227082868Z",
     "start_time": "2023-09-24T22:34:21.397940520Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have formed two clusters. The pattern of the two centroids seems fairly similar,\n",
    "and the separation of clusters does not seem very good. We can score the clustering\n",
    "with the score method, which by default returns `self.inertia_`, which is a measure\n",
    "of between cluster variation used as a stopping condition.\n",
    "For a range of clusteirng comparison algorithms, see the [sklearn clustering API]\n",
    "(https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)\n",
    "\n",
    "Low intertia is better, so to conform to the sklearn interface which prefers to\n",
    "maximize performance criteria, inertia scores are negative.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "s1 = k_means.score(X_test, y_test)\n",
    "s1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:23.444766078Z",
     "start_time": "2023-09-24T22:34:23.431834602Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    " We think that best default configuration for  $k$-means is the msm metric with\n",
    "barycentre averaging (see [1] for experimental support for this). We can configure\n",
    "this as such:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Best configuration for k-means\n",
    "k_means = TimeSeriesKMeans(\n",
    "    n_clusters=2,  # Number of desired centers\n",
    "    init_algorithm=\"random\",  # Center initialisation technique\n",
    "    max_iter=10,  # Maximum number of iterations for refinement on training set\n",
    "    distance=\"msm\",  # Distance metric to use\n",
    "    averaging_method=\"ba\",  # Averaging technique to use\n",
    "    random_state=1,\n",
    "    average_params={\n",
    "        \"distance\": \"msm\",\n",
    "    },\n",
    ")\n",
    "\n",
    "k_means.fit(X_train)\n",
    "plot_cluster_algorithm(k_means, X_test, k_means.n_clusters)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:30.830487631Z",
     "start_time": "2023-09-24T22:34:25.067991040Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have formed two clusters with barycentre averaging using MSM distance. The centroids\n",
    "seem more distinct in shape now, with the first cluster centroid looking much\n",
    "flatter than the second. The inertia has been reduced, so the score is higher."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "s2 = k_means.score(X_test, y_test)\n",
    "s2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:32.942535996Z",
     "start_time": "2023-09-24T22:34:32.927123024Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. K-medoids clustering\n",
    "$k$-medoids is another popular TSCL approach. The goal is to partition n\n",
    "observations into $k$ clusters in which each observation belongs to the cluster with\n",
    "the nearest medoid/centroid. There are two main variants which are PAM and alternate.\n",
    " PAM is the default and is the most accurate but is slower than alternate [3]. The\n",
    " best distance to use with all variants of k-medoids is MSM [1][3]. Basic usage is\n",
    " as follows:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "temp2 = TimeSeriesKMedoids(\n",
    "    distance=\"msm\",  # MSM distance with c parameter set to 0.2 and 90% window.\n",
    "    distance_params={\"c\": 2.0, \"window\": 0.9, \"independent\": True},\n",
    ")\n",
    "print(temp.distance, \", \", temp.distance_params)\n",
    "print(temp2.distance, \", \", temp2.distance_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:34.669402543Z",
     "start_time": "2023-09-24T22:34:34.655405233Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are several variants of medoid based clustering algorithms in aeon:\n",
    "\n",
    "3.1. Partitioning Around the Medoids (PAM): class `TimeSeriesKMedoids`, method = `pam`\n",
    "\n",
    "3.2. Alternating $k$-medoids: class `TimeSeriesKMedoids`, method = `alternate`\n",
    "\n",
    "3.3. Clustering LARge Applications (CLARA): class `TimeSeriesCLARA`\n",
    "\n",
    "3.4. CLARA based raNdomised Search (CLARANS): class `TimeSeriesCLARANS`\n",
    "\n",
    "We start with an example showing the important parameters and using DTW distance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "k_medoids = TimeSeriesKMedoids(\n",
    "    n_clusters=2,  # Number of desired centers\n",
    "    init_algorithm=\"random\",  # Center initialisation technique\n",
    "    max_iter=10,  # Maximum number of iterations for refinement on training set\n",
    "    verbose=False,  # Verbose\n",
    "    distance=\"dtw\",  # Distance to use\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "k_medoids.fit(X_train)\n",
    "s3 = k_medoids.score(X_test, y_test)\n",
    "plot_cluster_algorithm(k_medoids, X_test, k_medoids.n_clusters)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:36.285936377Z",
     "start_time": "2023-09-24T22:34:36.003560286Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Partitioning Around the Medoids (PAM): class `TimeSeriesKMedoids`, method =\n",
    "`pam`\n",
    "\n",
    "The  main distinguishing factor  of PAM is that in the rassigning of representative\n",
    "centres (medoids) it allows cases to become medoids of clusters they are not\n",
    "currently members of. This reduces the risk of premature convergence. Trying out all\n",
    "case combinations of medoids is exponential in time complexity, so PAM employs an\n",
    "heuristic. WE demonstrate the"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above shows the basic usage for K-medoids. `TimeSeriesKMedoids` uses\n",
    "the partition around medoids (PAM) algorithm by default to update the centres.\n",
    "The parameter key to k-medoids is the distance and is what we\n",
    "can adjust to improve performance for time series. We have found that using MSM with\n",
    "PAM produces better clusters on average [1]:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "k_medoids = TimeSeriesKMedoids(\n",
    "    n_clusters=2,  # Number of desired centers\n",
    "    init_algorithm=\"random\",  # Center initialisation technique\n",
    "    max_iter=10,  # Maximum number of iterations for refinement on training set\n",
    "    distance=\"msm\",  # Distance to use\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "k_medoids.fit(X_train)\n",
    "s4 = k_medoids.score(X_test, y_test)\n",
    "plot_cluster_algorithm(k_medoids, X_test, k_medoids.n_clusters)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:39.683463551Z",
     "start_time": "2023-09-24T22:34:39.303479519Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(f\" PAM DTW score {s3} PAM MSM score {s4}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:40.530040710Z",
     "start_time": "2023-09-24T22:34:40.499052460Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Alternate k-medoids\n",
    "<p>In addition there is another popular way of performing k-medoids which is\n",
    "using the alternate k-medoids algorithm. This method adapts the Lloyd's algorithm\n",
    "(used in k-means) but instead of using the mean average to update the centres\n",
    "it uses the medoid. This method is generally faster than PAM however it tends to be less\n",
    "accurate. This method can be used by parsing 'alternate' as the parameter to\n",
    "TimeSeriesKMedoids.</p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "k_medoids = TimeSeriesKMedoids(\n",
    "    n_clusters=2,  # Number of desired centers\n",
    "    init_algorithm=\"random\",  # Center initialisation technique\n",
    "    max_iter=10,  # Maximum number of iterations for refinement on training set\n",
    "    distance=\"msm\",  # Distance to use\n",
    "    random_state=1,\n",
    "    method=\"alternate\",\n",
    ")\n",
    "\n",
    "k_medoids.fit(X_train)\n",
    "s5 = k_medoids.score(X_test, y_test)\n",
    "plot_cluster_algorithm(k_medoids, X_test, k_medoids.n_clusters)\n",
    "print(\"Alternate MSM score = \", s5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:42.225573865Z",
     "start_time": "2023-09-24T22:34:41.982075440Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 3.3 Clustering LARge Applications (CLARA): class `TimeSeriesCLARA`</h3>\n",
    "<p> Clustering LARge Applications (CLARA)  [4]_ improves the run time performance of\n",
    "PAM by only performing PAM on a subsample of the the dataset to generate the\n",
    "cluster centres. This greatly improves the time taken to train the model however,\n",
    "degrades the quality of the clusters.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from aeon.clustering import TimeSeriesCLARA\n",
    "\n",
    "clara = TimeSeriesCLARA(\n",
    "    n_clusters=2,  # Number of desired centers\n",
    "    max_iter=10,  # Maximum number of iterations for refinement on training set\n",
    "    distance=\"msm\",  # Distance to use\n",
    "    random_state=1,\n",
    ")\n",
    "clara.fit(X_train)\n",
    "s6 = k_medoids.score(X_test, y_test)\n",
    "plot_cluster_algorithm(clara, X_test, clara.n_clusters)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:43.974814253Z",
     "start_time": "2023-09-24T22:34:43.492140492Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4. CLARA based raNdomised Search (CLARANS): class `TimeSeriesCLARANS`\n",
    "<p>    CLARA based raNdomised Search (CLARANS) [4] tries to improve the run time of\n",
    "PAM by adapting the swap operation of PAM to\n",
    "    use a more greedy approach. This is done by only performing the first swap which\n",
    "    results in a reduction in total deviation before continuing evaluation. It limits\n",
    "    the number of attempts known as max neighbours to randomly select and check if\n",
    "    total deviation is reduced. This random selection gives CLARANS an advantage when\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from aeon.clustering import TimeSeriesCLARANS\n",
    "\n",
    "clara = TimeSeriesCLARANS(\n",
    "    n_clusters=2,  # Number of desired centers\n",
    "    distance=\"msm\",  # Distance to use\n",
    "    random_state=1,\n",
    ")\n",
    "clara.fit(X_train)\n",
    "s7 = k_medoids.score(X_test, y_test)\n",
    "plot_cluster_algorithm(clara, X_test, clara.n_clusters)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:45.178687870Z",
     "start_time": "2023-09-24T22:34:44.918595172Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(f\" Clara score {s6} Clarans score = {s7}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:45.832186852Z",
     "start_time": "2023-09-24T22:34:45.800973568Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance comparison\n",
    "\n",
    "We conducted a comparison of $k$-medoids and $k$-means with different distance\n",
    "functions in [1][2][3]. We commonly use critical difference diagrams to compare\n",
    "performance. The headline conclusions were\n",
    "\n",
    "1. MSM is the most effective distance function [1]\n",
    "2. PAM is on average better alternate for $k$-medoids\n",
    "3. Barycentre averaging with MSM is better than using DBA [2]\n",
    "4. $k$-medoids is generally better than $k$-means [3]\n",
    "\n",
    "All the results to support these conclusions are (or will be) avaialable on the\n",
    "aeon companion site [tsml archive](https://www.timeseriesclassification.com/). We can\n",
    "download these results and recreate the performance comparison in `aeon`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from aeon.benchmarking.results_loaders import get_estimator_results_as_array\n",
    "from aeon.visualisation import plot_critical_difference\n",
    "\n",
    "# 1. MSM is the most effective distance function\n",
    "clusterers = [\n",
    "    \"kmeans-ed\",\n",
    "    \"kmeans-dtw\",\n",
    "    \"kmeans-msm\",\n",
    "    \"kmeans-twe\",\n",
    "    \"kmedoids-ed\",\n",
    "    \"kmedoids-dtw\",\n",
    "    \"kmedoids-msm\",\n",
    "    \"kmedoids-twe\",\n",
    "]\n",
    "accuracy, data_names = get_estimator_results_as_array(\n",
    "    task=\"clustering\", estimators=clusterers, measure=\"clacc\"\n",
    ")\n",
    "print(f\" Returned results in shape {accuracy.shape}\")\n",
    "plt = plot_critical_difference(accuracy, clusterers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:34:48.704606579Z",
     "start_time": "2023-09-24T22:34:47.860234094Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have pulled down the test accuracies from the website for eight clustering\n",
    "algorithms for 111 datsets. the number of above each line is the average rank of each\n",
    " algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### References\n",
    "\n",
    "[1] Christopher Holder, Matthew Middlehurst, and Anthony Bagnall. A Review and\n",
    "Evaluation of Elastic Distance Functions for Time Series Clustering,  Knowledge and Information Systems. In Press (2023)\n",
    "\n",
    "[2] Christopher Holder, David Guijo-Rubio, and Anthony Bagnall. Barycentre averaging\n",
    "for the move-split-merge time series distance measure. 15th International Joint\n",
    "Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (2023)\n",
    "\n",
    "[3] Christopher Holder, David Guijo-Rubio, and Anthony Bagnall. \"Clustering time\n",
    "series with k-medoids based algorithms\" 8th Workshop on Advanced Analytics and\n",
    "Learning on Temporal Data (AALTD) at ECML-PKDD (2023)\n",
    "\n",
    "[4] Kaufman, Leonard & Rousseeuw, Peter. (1986). Clustering Large Data Sets. 10.1016/B978-0-444-87877-9.50039-X.\n",
    "\n",
    "[5] R. T. Ng and Jiawei Han, \"CLARANS: a method for clustering objects spatial data mining,\" in IEEE Transactions on Knowledge and Data Engineering vol. 14, no. 5, pp. 1003-1016, Sept.-Oct. 2002, doi: 10.1109/TKDE.2002.1033770.\n",
    "\n",
    "[6] F. Petitjean, A. Ketterlin and P. Gancarski, \"A global averaging method for dynamic time warping,\n",
    "with applications to clustering\", Pattern Recognition, vol. 44, pp. 678-693, 2011\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
